{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4e5940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, argparse, random, pandas as pd, numpy as np, os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f7f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAND \n",
    "def evaluate_rand(df, topn, neg_k=19):\n",
    "    \"\"\"RAND baseline: random shuffle candidate set\"\"\"\n",
    "    rng = np.random.default_rng(42); results = {}\n",
    "    for n in topn:\n",
    "        hit = tot = 0\n",
    "        for _, grp in df.groupby('guest_id'):\n",
    "            pos, neg = grp[grp.label==1], grp[grp.label==0]\n",
    "            if pos.empty or neg.empty: continue\n",
    "            for idx in pos.index:\n",
    "                sel = rng.choice(neg.index, size=min(neg_k, len(neg)), replace=False)\n",
    "                cand = np.append(sel, idx); rng.shuffle(cand)\n",
    "                if idx in cand[:n]: hit += 1\n",
    "                tot += 1\n",
    "        results[f'HR@{n}'] = hit / tot if tot else 0.0\n",
    "    return results\n",
    "\n",
    "#  STL (TF-IDF + LR) \n",
    "class STL:\n",
    "    def __init__(self, max_feat=50):\n",
    "        self.vec = TfidfVectorizer(max_features=max_feat, stop_words='english', min_df=5, max_df=0.5)\n",
    "        self.clf = LogisticRegression(max_iter=50, random_state=42)\n",
    "        self.fit_done = False\n",
    "    def _txt(self, df, idict):\n",
    "        return [str(idict.get(lid, '')) for lid in df.listing_id]\n",
    "    def _X(self, texts):\n",
    "        X = self.vec.transform(texts) if self.fit_done else self.vec.fit_transform(texts)\n",
    "        self.fit_done = True; return X\n",
    "    def fit(self, df, idict):\n",
    "        self.clf.fit(self._X(self._txt(df, idict)), df.label)\n",
    "        return self\n",
    "    def score(self, df, idict):\n",
    "        return self.clf.predict_proba(self._X(self._txt(df, idict)))[:,1]\n",
    "\n",
    "def evaluate_stl(tr, te, idict, topn, neg_k=19):\n",
    "    model = STL().fit(tr, idict)\n",
    "    tmp = te.copy(); tmp['score'] = model.score(tmp, idict)\n",
    "    results = {}\n",
    "    for n in topn:\n",
    "        hit = tot = 0\n",
    "        for _, grp in tmp.groupby('guest_id'):\n",
    "            pos, neg = grp[grp.label==1], grp[grp.label==0]\n",
    "            if pos.empty or neg.empty: continue\n",
    "            for idx in pos.index:\n",
    "                sel = np.random.choice(neg.index, min(neg_k,len(neg)), replace=False)\n",
    "                cand = pd.concat([neg.loc[sel], pos.loc[[idx]]]).sort_values('score', ascending=False)\n",
    "                if idx in cand.head(n).index: hit += 1\n",
    "                tot += 1\n",
    "        results[f'HR@{n}'] = hit / tot if tot else 0\n",
    "    return results\n",
    "\n",
    "# RTM‑G baseline \n",
    "class RTM_G:\n",
    "    def __init__(self, n_topics: int = 60, lda_iter: int = 5):\n",
    "        self.cv = CountVectorizer(stop_words='english', min_df=5, max_df=0.5)\n",
    "        self.lda = LatentDirichletAllocation(n_components=n_topics, max_iter=lda_iter,\n",
    "                                             learning_method='batch', random_state=0)\n",
    "        self.clf = LogisticRegression(max_iter=50, random_state=42)\n",
    "        self.cv_fitted = False\n",
    "\n",
    "    def _txt(self, df: pd.DataFrame, idict: dict[str, str]) -> list[str]:\n",
    "        return [str(idict.get(lid, '')) for lid in df.listing_id]\n",
    "\n",
    "    def _theta(self, texts: list[str]):\n",
    "        X = self.cv.transform(texts) if self.cv_fitted else self.cv.fit_transform(texts)\n",
    "        self.cv_fitted = True\n",
    "        return self.lda.transform(X) if hasattr(self.lda, 'components_') else self.lda.fit_transform(X)\n",
    "\n",
    "    def fit(self, df: pd.DataFrame, idict: dict[str, str]):\n",
    "        self.clf.fit(self._theta(self._txt(df, idict)), df.label)\n",
    "        return self\n",
    "\n",
    "    def score(self, df: pd.DataFrame, idict: dict[str, str]):\n",
    "        return self.clf.predict_proba(self._theta(self._txt(df, idict)))[:, 1]\n",
    "\n",
    "def evaluate_rtm_g(tr, te, idict, topn, neg_k=19):\n",
    "    model = RTM_G().fit(tr, idict)\n",
    "    tmp = te.copy(); tmp['score'] = model.score(tmp, idict)\n",
    "    results = {}\n",
    "    for n in topn:\n",
    "        hit = tot = 0\n",
    "        for _, grp in tmp.groupby('guest_id'):\n",
    "            pos, neg = grp[grp.label==1], grp[grp.label==0]\n",
    "            if pos.empty or neg.empty: continue\n",
    "            for idx in pos.index:\n",
    "                sel = np.random.choice(neg.index, min(neg_k,len(neg)), replace=False)\n",
    "                cand = pd.concat([neg.loc[sel], pos.loc[[idx]]]).sort_values('score', ascending=False)\n",
    "                if idx in cand.head(n).index: hit += 1\n",
    "                tot += 1\n",
    "        results[f'HR@{n}'] = hit / tot if tot else 0\n",
    "    return results\n",
    "\n",
    "# Main Callable Logic \n",
    "def run_baselines(data_root='autodl-fs', topn=range(1, 11), neg_k=19):\n",
    "    root = Path(data_root)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Data directory '{root}' not found\")\n",
    "    tr = pd.read_csv(root / 'transaction_train.csv')\n",
    "    te = pd.read_csv(root / 'transaction_test.csv')\n",
    "    idict = dict(pd.read_csv(root / 'dj_documents_unique.csv').values)\n",
    "\n",
    "    all_res = {}\n",
    "    print('Evaluating RAND...');   all_res['RAND']   = evaluate_rand(te, topn, neg_k)\n",
    "    print('Evaluating STL...');    all_res['STL']    = evaluate_stl(tr, te, idict, topn, neg_k)\n",
    "    print('Evaluating RTM-G...');  all_res['RTM-G']  = evaluate_rtm_g(tr, te, idict, topn, neg_k)\n",
    "\n",
    "    # Output table\n",
    "    header = 'TopN | ' + ' | '.join(f'{k:7s}' for k in all_res)\n",
    "    print('\\n' + header)\n",
    "    print('-'*len(header))\n",
    "    for n in topn:\n",
    "        row = f' {n:>2}  | ' + ' | '.join(f\"{all_res[k][f'HR@{n}']:.3f}\" for k in all_res)\n",
    "        print(row)\n",
    "\n",
    "    # Save results\n",
    "    out_dir = Path('brtm_outputs')\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    pd.DataFrame(all_res).T.to_csv(out_dir / 'baseline_results.csv')\n",
    "    print(f\"\\nResults saved to: {out_dir / 'baseline_results.csv'}\")\n",
    "    return all_res\n",
    "\n",
    "\n",
    "def print_baseline_comparison(final_results: dict, csv_path='brtm_outputs/baseline_results.csv'):\n",
    "    \"\"\"Compare RAND / STL / RTM‑G with your model (BRTM-Sample) using HR@1-10\"\"\"\n",
    "    baseline_df = pd.read_csv(csv_path, index_col=0)\n",
    "    rand = [baseline_df.loc['RAND', f'HR@{n}'] for n in range(1, 11)]\n",
    "    stl  = [baseline_df.loc['STL',  f'HR@{n}'] for n in range(1, 11)]\n",
    "    rtmg = [baseline_df.loc['RTM-G',f'HR@{n}'] for n in range(1, 11)]\n",
    "    brtm = [final_results.get(f'HR@{n}', 0.0) for n in range(1, 11)]\n",
    "    print(\"=\" * 90)\n",
    "    print(\"RAND vs STL vs RTM‑G vs YOURS (Hit Rate)\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"TopN | RAND |  STL  | RTM‑G | BRTM-SAMPLE | ∆ vs RTM‑G\")\n",
    "    print(\"-\" * 90)\n",
    "\n",
    "    for n in range(1, 11):\n",
    "        hr_rand = rand[n - 1]\n",
    "        hr_stl  = stl[n - 1]\n",
    "        hr_rtmg = rtmg[n - 1]\n",
    "        hr_ours = brtm[n - 1]\n",
    "        improve = ((hr_ours - hr_rtmg) / hr_rtmg * 100) if hr_rtmg else 0.0\n",
    "        print(f\" {n:>2}  | {hr_rand:.3f} | {hr_stl:.3f} | {hr_rtmg:.3f} | {hr_ours:.3f} | {improve:6.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a605617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hit_rates(npz_path='brtm_outputs/brtm_table7_complete_results.npz') -> dict:\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    return data['hit_rates'].item()\n",
    "final_results = load_hit_rates('brtm_outputs/brtm_table7_complete_results.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c11c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAND...\n",
      "Evaluating STL...\n",
      "Evaluating RTM-G...\n",
      "\n",
      "TopN | RAND    | STL     | RTM-G  \n",
      "----------------------------------\n",
      "  1  | 0.060 | 0.291 | 0.335\n",
      "  2  | 0.115 | 0.383 | 0.469\n",
      "  3  | 0.155 | 0.461 | 0.548\n",
      "  4  | 0.206 | 0.534 | 0.595\n",
      "  5  | 0.225 | 0.592 | 0.623\n",
      "  6  | 0.291 | 0.652 | 0.673\n",
      "  7  | 0.356 | 0.702 | 0.702\n",
      "  8  | 0.391 | 0.751 | 0.746\n",
      "  9  | 0.450 | 0.786 | 0.798\n",
      " 10  | 0.469 | 0.818 | 0.840\n",
      "\n",
      "Results saved to: brtm_outputs/baseline_results.csv\n"
     ]
    }
   ],
   "source": [
    "# run baseline\n",
    "baselines = run_baselines(data_root='autodl-fs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1643dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "RAND vs STL vs RTM‑G vs YOURS (Hit Rate)\n",
      "==========================================================================================\n",
      "TopN | RAND |  STL  | RTM‑G | BRTM-SAMPLE | ∆ vs RTM‑G\n",
      "------------------------------------------------------------------------------------------\n",
      "  1  | 0.060 | 0.291 | 0.335 | 0.300 |  -10.4%\n",
      "  2  | 0.115 | 0.383 | 0.469 | 0.434 |   -7.5%\n",
      "  3  | 0.155 | 0.461 | 0.548 | 0.529 |   -3.6%\n",
      "  4  | 0.206 | 0.534 | 0.595 | 0.566 |   -4.8%\n",
      "  5  | 0.225 | 0.592 | 0.623 | 0.645 |    3.5%\n",
      "  6  | 0.291 | 0.652 | 0.673 | 0.696 |    3.5%\n",
      "  7  | 0.356 | 0.702 | 0.702 | 0.769 |    9.5%\n",
      "  8  | 0.391 | 0.751 | 0.746 | 0.800 |    7.2%\n",
      "  9  | 0.450 | 0.786 | 0.798 | 0.843 |    5.7%\n",
      " 10  | 0.469 | 0.818 | 0.840 | 0.857 |    2.0%\n"
     ]
    }
   ],
   "source": [
    "# compare with BRTM-Sample results\n",
    "print_baseline_comparison(final_results, csv_path='brtm_outputs/baseline_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe9838-2225-41a5-bbb5-75c1f276e8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
